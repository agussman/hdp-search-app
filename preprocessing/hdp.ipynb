{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim, os, re\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txts_dir = \"./txt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "index = \"arxiv\"\n",
    "\n",
    "if es.indices.exists(index):\n",
    "    es.indices.delete(index)\n",
    "es.indices.create(index)\n",
    "\n",
    "for file_name in os.listdir(txts_dir):\n",
    "    with open(txts_dir + file_name, 'r') as f:\n",
    "        text = f.read()\n",
    "    clean_txt = \" \".join(clean_text(text))\n",
    "    name = file_name.replace(\".txt\", \"\")\n",
    "    body = { \"name\" : name, \"text\" : clean_txt }\n",
    "    es.index(index=index, doc_type=\"file\", body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set([ \n",
    "    \"by\", \"the\", \"of\", \"for\", \"and\", \"in\", \"to\", \n",
    "    \"over\", \"or\", \"with\", \"under\", \"no\", \"not\", \n",
    "    \"from\", \"at\", \"as\", \"without\", \"this\", \"who\",\n",
    "    \"other\", \"they\", \"them\", \"also\", \"except\", \"on\",\n",
    "    \"are\", \"is\", \"that\", \"is\", \"were\", \"was\", \"had\",\n",
    "    \"that\", \"which\", \"it\", \"be\", \"use\", \"generally\",\n",
    "    \"than\", \"through\", \"via\", \"between\", \"each\",\n",
    "    \"those\", \"these\", \"have\", \"any\", \"now\", \"if\", \"should\",\n",
    "    \"such\", \"has\", \"what\", \"into\", \"primarily\", \n",
    "    \"more\", \"comprises\", \"all\", \"can\", \"ie\", \"what\", \n",
    "    \"below\", \"see\", \"about\", \"its\", \"eg\", \"greater\", \n",
    "    \"their\", \"among\", \"after\", \"having\",\"while\", \"an\", \"we\", \n",
    "     \"here\", \"you\", \"will\", \"your\", \"only\", \"likely\", \n",
    "    \"because\", \"etc\", \"shall\", \"his\", \"her\", \"ever\", \n",
    "    \"every\", \"then\", \"within\", \"likewise\", \"onto\"\n",
    "])\n",
    "\n",
    "def clean_text(txt):    \n",
    "    clean_words = []\n",
    "    for w in txt.split():\n",
    "        if w not in stop_words and re.search(r\"^[A-Za-z].*[A-Za-z]$\", w):\n",
    "            w = re.sub(r\"[^A-Za-z-]\", \"\", w)\n",
    "            if len(w) > 1:\n",
    "                clean_words.append(w.lower())\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing: hdp_topics.txt\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for f in os.listdir(txts_dir):\n",
    "    with open(txts_dir + f,'r') as f:\n",
    "        docs.append(f.read())\n",
    "        \n",
    "texts = [clean_text(d) for d in docs]\n",
    "id2word = gensim.corpora.Dictionary(texts)\n",
    "id2word.filter_extremes(no_below=10,keep_n=100000)\n",
    "id2word.compactify()\n",
    "id2word.save(\"hdpDictionary\")\n",
    "id2freq = [id2word.doc2bow(t) for t in texts]\n",
    "\n",
    "hdp = gensim.models.hdpmodel.HdpModel(corpus=id2freq, id2word=id2word)\n",
    "hdp.save(\"hdpModel\")\n",
    "\n",
    "topics_file = \"hdp_topics.txt\"\n",
    "topics = hdp.show_topics(topics=-1, topn=15)\n",
    "with open(topics_file, \"w\") as f:\n",
    "    for topic in topics:\n",
    "        f.write(\"{}\\n\".format(topic))\n",
    "print \"done processing: \" + topics_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "topic2words = dict()\n",
    "for topic in topics:\n",
    "    start = topic.index(\" \") + 1\n",
    "    end = topic.index(\":\")\n",
    "    topic_num = topic[start:end]\n",
    "    pairs = topic[end+1:-1]\n",
    "    word2weight = dict()\n",
    "    for pair in pairs.split(\"+\"):\n",
    "        weight, word = pair.strip().split(\"*\")\n",
    "        word2weight[word] = float(weight)\n",
    "    topic2words[topic_num] = word2weight\n",
    "\n",
    "with open(\"topic2words\", \"w\") as f:\n",
    "    pickle.dump(topic2words, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
